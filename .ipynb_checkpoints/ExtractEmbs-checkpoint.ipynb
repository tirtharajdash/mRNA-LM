{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b397f01b-c277-43a1-8a54-a9211be797dd",
   "metadata": {},
   "source": [
    "This notebook describes how to use mRNA-LM (individual models) for extracting embeddings from saved (pretrained) models: CodonBERT, 5' and 3' UTRBERTS. \n",
    "\n",
    "- First download the pretrained models from the mentioned links (see [README](https://github.com/Sanofi-Public/mRNA-LM/tree/main?tab=readme-ov-file#pre-trained-models)). We save them inside `pretrained_models/`\n",
    "- Unzip the model file: `unzip <model_file.zip>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2204e5c5-ae9d-4124-93eb-dc4afad5d16b",
   "metadata": {},
   "source": [
    "#### CDS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51529cee-02ff-46fd-9c8e-0048f2d62ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(69, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(1024, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "model_path = \"./pretrained_models/codonbert\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_path, local_files_only=True)\n",
    "cds_model = AutoModel.from_pretrained(model_path, config=config, local_files_only=True)\n",
    "print(cds_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "098177c2-c705-448e-a524-3c80e4cd2407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./codon_tokenizer/tokenizer_config.json',\n",
       " './codon_tokenizer/special_tokens_map.json',\n",
       " './codon_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CDS Tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "def build_standard_codon_tokenizer():\n",
    "    bases = ['A', 'U', 'G', 'C']\n",
    "    special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    "    codons = [a + b + c for a in bases for b in bases for c in bases]  # 4^3 = 64 codons\n",
    "    vocab = special_tokens + codons\n",
    "    vocab_dict = {tok: i for i, tok in enumerate(vocab)}\n",
    "\n",
    "    tokenizer = Tokenizer(WordLevel(vocab=vocab_dict, unk_token='[UNK]'))\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer.post_processor = BertProcessing(\n",
    "        (\"[SEP]\", vocab_dict[\"[SEP]\"]),\n",
    "        (\"[CLS]\", vocab_dict[\"[CLS]\"])\n",
    "    )\n",
    "\n",
    "    return PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        unk_token='[UNK]',\n",
    "        sep_token='[SEP]',\n",
    "        pad_token='[PAD]',\n",
    "        cls_token='[CLS]',\n",
    "        mask_token='[MASK]'\n",
    "    )\n",
    "\n",
    "tokenizer = build_standard_codon_tokenizer()\n",
    "\n",
    "\n",
    "assert len(tokenizer.get_vocab()) == 69\n",
    "assert max(tokenizer.get_vocab().values()) == 68\n",
    "\n",
    "\n",
    "tokenizer.save_pretrained(\"./codon_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc92030a-d1d7-4038-b24e-2a76785e3e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BatchEncoding\n",
    "from typing import List, Union\n",
    "\n",
    "def preprocess_CDS_sequence(seq: str) -> str:\n",
    "    \"\"\"Convert DNA to RNA and split into codons.\"\"\"\n",
    "    seq = seq.replace(\"T\", \"U\")\n",
    "    codons = [seq[i:i+3] for i in range(0, len(seq), 3) if len(seq[i:i+3]) == 3]\n",
    "    return ' '.join(codons)\n",
    "\n",
    "def batch_tokenize(sequences: List[str], tokenizer, max_length=1024) -> BatchEncoding:\n",
    "    \"\"\"Preprocess and tokenize a list of RNA sequences.\"\"\"\n",
    "    processed = [preprocess_CDS_sequence(seq) for seq in sequences]\n",
    "    return tokenizer(\n",
    "        processed,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "def extract_CDS_embeddings_batch(sequences: List[str], tokenizer, model, mode='cls', batch_size=8) -> List[torch.Tensor]:\n",
    "    \"\"\"Extract embeddings for a list of sequences in batches.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    # Create mini-batches\n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        batch_seqs = sequences[i:i+batch_size]\n",
    "        inputs = batch_tokenize(batch_seqs, tokenizer)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state  # [B, T, H]\n",
    "\n",
    "            if mode == 'cls':\n",
    "                batch_embeddings = last_hidden[:, 0, :]  # [B, H]\n",
    "            elif mode == 'mean':\n",
    "                mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden.size())\n",
    "                masked = last_hidden * mask\n",
    "                sum_embeddings = masked.sum(dim=1)\n",
    "                lengths = mask.sum(dim=1).clamp(min=1e-9)\n",
    "                batch_embeddings = sum_embeddings / lengths  # [B, H]\n",
    "            else:\n",
    "                raise ValueError(\"mode must be 'cls' or 'mean'\")\n",
    "\n",
    "            embeddings.extend(batch_embeddings.cpu())\n",
    "\n",
    "    return embeddings  # List of [hidden_size] tensors\n",
    "\n",
    "def build_CDS_embedding_dataframe(sequences: Union[List[str], None] = None,\n",
    "                                   csv_path: Union[str, None] = None,\n",
    "                                   column: Union[str, None] = None,\n",
    "                                   mode: str = 'cls',\n",
    "                                   tokenizer=None,\n",
    "                                   model=None,\n",
    "                                   batch_size: int = 8) -> pd.DataFrame:\n",
    "    \"\"\"Return a DataFrame with sequences and their embeddings.\"\"\"\n",
    "    if csv_path:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        sequences = df[column].tolist()\n",
    "    elif sequences is None:\n",
    "        sequences = [\n",
    "            \"ATGCGATTTTCTAAAGTAAATGTT\",\n",
    "            \"ATGCCCGGGAAATTAGCTAA\",\n",
    "            \"ATGAAATTTCCCGGGTTTAA\",\n",
    "            \"ATGATATATATATGCGCGCGCGC\",\n",
    "            \"ATGCGCGTATATATATATAGTAG\"\n",
    "        ]\n",
    "\n",
    "    embeddings = extract_CDS_embeddings_batch(sequences, tokenizer, model, mode, batch_size)\n",
    "    return pd.DataFrame({\n",
    "        \"sequence\": sequences,\n",
    "        \"embedding\": [emb.numpy() for emb in embeddings]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e75d9e5-1207-427a-824a-ab948616e5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATGCGATTTTCTAAAGTAAATGTT</td>\n",
       "      <td>[0.26536927, -0.4860135, 0.10935764, -0.181523...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATGCCCGGGAAATTAGCTAA</td>\n",
       "      <td>[0.42503604, -0.41654238, 0.13327605, -0.00635...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATGAAATTTCCCGGGTTTAA</td>\n",
       "      <td>[0.13614827, -0.14827953, 0.17040502, -0.17318...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATGATATATATATGCGCGCGCGC</td>\n",
       "      <td>[0.5961642, -0.2065507, 0.15518683, -0.1861409...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATGCGCGTATATATATATAGTAG</td>\n",
       "      <td>[0.49323305, -0.3121234, 0.2550852, 0.00702007...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sequence                                          embedding\n",
       "0  ATGCGATTTTCTAAAGTAAATGTT  [0.26536927, -0.4860135, 0.10935764, -0.181523...\n",
       "1      ATGCCCGGGAAATTAGCTAA  [0.42503604, -0.41654238, 0.13327605, -0.00635...\n",
       "2      ATGAAATTTCCCGGGTTTAA  [0.13614827, -0.14827953, 0.17040502, -0.17318...\n",
       "3   ATGATATATATATGCGCGCGCGC  [0.5961642, -0.2065507, 0.15518683, -0.1861409...\n",
       "4   ATGCGCGTATATATATATAGTAG  [0.49323305, -0.3121234, 0.2550852, 0.00702007..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of CDS embedding: 768\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "df = build_CDS_embedding_dataframe(mode=\"mean\", tokenizer=tokenizer, model=cds_model, batch_size=4)\n",
    "display(df.head())\n",
    "print(f\"Length of CDS embedding: {len(df['embedding'][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174af99-1a1e-4671-ba8d-be0efead93cb",
   "metadata": {},
   "source": [
    "#### 3' and 5' UTR models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a5002f-1f26-4e79-83b4-116c0f5c1c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "dict_values([2, 0, 5, 6, 1, 9, 7, 3, 8, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./utr_tokenizer/tokenizer_config.json',\n",
       " './utr_tokenizer/special_tokens_map.json',\n",
       " './utr_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UTR tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "def build_nt_tokenizer():\n",
    "    special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    "    nucleotides = list(\"AUGCN\")  # RNA bases\n",
    "    vocab = special_tokens + nucleotides\n",
    "    vocab_dict = {token: idx for idx, token in enumerate(vocab)}\n",
    "    \n",
    "    tokenizer = Tokenizer(WordLevel(vocab=vocab_dict, unk_token='[UNK]'))\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer.post_processor = BertProcessing(\n",
    "        (\"[SEP]\", vocab_dict[\"[SEP]\"]),\n",
    "        (\"[CLS]\", vocab_dict[\"[CLS]\"])\n",
    "    )\n",
    "    \n",
    "    utr_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        unk_token='[UNK]',\n",
    "        sep_token='[SEP]',\n",
    "        pad_token='[PAD]',\n",
    "        cls_token='[CLS]',\n",
    "        mask_token='[MASK]'\n",
    "    )\n",
    "\n",
    "    return utr_tokenizer\n",
    "\n",
    "utr_tokenizer = build_nt_tokenizer()\n",
    "\n",
    "print(len(utr_tokenizer.get_vocab()))\n",
    "print(utr_tokenizer.get_vocab().values())\n",
    "\n",
    "assert len(utr_tokenizer.get_vocab()) == 10\n",
    "assert max(utr_tokenizer.get_vocab().values()) == 9\n",
    "\n",
    "utr_tokenizer.save_pretrained(\"./utr_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79ec0914-8e3b-4b9d-ab19-7e53f8d99f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from typing import List, Union\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "def preprocess_UTR_sequence(seq: str) -> str:\n",
    "    \"\"\"Convert DNA to RNA and add whitespace between bases.\"\"\"\n",
    "    seq = seq.replace(\"T\", \"U\")\n",
    "    return ' '.join(seq)\n",
    "\n",
    "def batch_tokenize_utr(sequences: List[str], tokenizer, max_length=1024) -> BatchEncoding:\n",
    "    \"\"\"Preprocess and tokenize a list of UTR sequences (single nucleotide tokens).\"\"\"\n",
    "    processed = [preprocess_UTR_sequence(seq) for seq in sequences]\n",
    "    return tokenizer(\n",
    "        processed,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "def extract_UTR_embeddings_batch(sequences: List[str], tokenizer, model, mode='cls', batch_size=8, max_length=1024) -> List[torch.Tensor]:\n",
    "    \"\"\"Extract UTR embeddings in batches.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        batch_seqs = sequences[i:i+batch_size]\n",
    "        inputs = batch_tokenize_utr(batch_seqs, tokenizer, max_length=max_length)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state  # [B, T, H]\n",
    "\n",
    "            if mode == 'cls':\n",
    "                batch_embeddings = last_hidden[:, 0, :]  # [B, H]\n",
    "            elif mode == 'mean':\n",
    "                mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden.size())\n",
    "                masked = last_hidden * mask\n",
    "                sum_embeddings = masked.sum(dim=1)\n",
    "                lengths = mask.sum(dim=1).clamp(min=1e-9)\n",
    "                batch_embeddings = sum_embeddings / lengths\n",
    "            else:\n",
    "                raise ValueError(\"mode must be 'cls' or 'mean'\")\n",
    "\n",
    "            embeddings.extend(batch_embeddings.cpu())\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def build_UTR_embedding_dataframe(region: str,\n",
    "                                  sequences: Union[List[str], None] = None,\n",
    "                                  csv_path: Union[str, None] = None,\n",
    "                                  column: Union[str, None] = None,\n",
    "                                  mode: str = 'cls',\n",
    "                                  tokenizer=None,\n",
    "                                  model=None,\n",
    "                                  batch_size: int = 8) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract UTR embeddings (5' or 3') and return a DataFrame.\n",
    "    \n",
    "    region: '5utr' or '3utr'\n",
    "    \"\"\"\n",
    "    assert region in [\"5utr\", \"3utr\"], \"Region must be '5utr' or '3utr'\"\n",
    "\n",
    "    max_len = 512 if region == \"5utr\" else 1024\n",
    "\n",
    "    if csv_path:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        sequences = df[column].tolist()\n",
    "    elif sequences is None:\n",
    "        sequences = [\n",
    "            \"ATGCGATTTTCTAAAGTAAATGTT\",\n",
    "            \"ATGCCCGGGAAATTAGCTAA\",\n",
    "            \"ATGAAATTTCCCGGGTTTAA\",\n",
    "            \"ATGATATATATATGCGCGCGCGC\",\n",
    "            \"ATGCGCGTATATATATATAGTAG\"\n",
    "        ]\n",
    "\n",
    "    embeddings = extract_UTR_embeddings_batch(\n",
    "        sequences, tokenizer, model, mode=mode, batch_size=batch_size, max_length=max_len\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"sequence\": sequences,\n",
    "        \"embedding\": [emb.numpy() for emb in embeddings]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e08eeb18-872b-4253-8346-0e720805eaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ./pretrained_models/mrna_5utr_model_p2_cp85600_best and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATGCGATTTTCTAAAGTAAATGTT</td>\n",
       "      <td>[0.35632232, 0.71432865, -0.7655516, 0.7875675...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATGCCCGGGAAATTAGCTAA</td>\n",
       "      <td>[-0.36231306, 0.30640456, -0.574126, 1.2636681...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATGAAATTTCCCGGGTTTAA</td>\n",
       "      <td>[0.25398535, 0.28713754, -0.19550385, 0.673237...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATGATATATATATGCGCGCGCGC</td>\n",
       "      <td>[0.67684096, -0.078162774, -0.52981377, 0.7569...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATGCGCGTATATATATATAGTAG</td>\n",
       "      <td>[0.3242923, 0.07677141, 0.14978021, 0.48897576...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sequence                                          embedding\n",
       "0  ATGCGATTTTCTAAAGTAAATGTT  [0.35632232, 0.71432865, -0.7655516, 0.7875675...\n",
       "1      ATGCCCGGGAAATTAGCTAA  [-0.36231306, 0.30640456, -0.574126, 1.2636681...\n",
       "2      ATGAAATTTCCCGGGTTTAA  [0.25398535, 0.28713754, -0.19550385, 0.673237...\n",
       "3   ATGATATATATATGCGCGCGCGC  [0.67684096, -0.078162774, -0.52981377, 0.7569...\n",
       "4   ATGCGCGTATATATATATAGTAG  [0.3242923, 0.07677141, 0.14978021, 0.48897576..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_path = \"./pretrained_models/mrna_5utr_model_p2_cp85600_best\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_path, local_files_only=True)\n",
    "model_5utr = AutoModel.from_pretrained(model_path, config=config, local_files_only=True)\n",
    "tokenizer_utr = AutoTokenizer.from_pretrained(\"./utr_tokenizer\") \n",
    "\n",
    "df_5utr = build_UTR_embedding_dataframe(\n",
    "    region=\"5utr\",\n",
    "    mode=\"mean\",\n",
    "    tokenizer=tokenizer_utr,\n",
    "    model=model_5utr,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "display(df_5utr)\n",
    "print(len(df_5utr['embedding'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a8a1e2-19f8-4abf-a7c4-9a58516320c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ./pretrained_models/mrna_3utr_model_p2_cp99900_best and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATGCGATTTTCTAAAGTAAATGTT</td>\n",
       "      <td>[-0.46467862, -0.50936043, 0.36862156, 0.86998...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATGCCCGGGAAATTAGCTAA</td>\n",
       "      <td>[-0.39854795, -0.74053574, -0.4361593, -0.2348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATGAAATTTCCCGGGTTTAA</td>\n",
       "      <td>[-0.037209284, -0.41736507, 0.18577996, 0.4326...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATGATATATATATGCGCGCGCGC</td>\n",
       "      <td>[-0.43701595, -0.052947313, -0.2544344, 0.2598...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATGCGCGTATATATATATAGTAG</td>\n",
       "      <td>[-0.8236446, -0.2006473, -0.6237884, 0.6729365...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sequence                                          embedding\n",
       "0  ATGCGATTTTCTAAAGTAAATGTT  [-0.46467862, -0.50936043, 0.36862156, 0.86998...\n",
       "1      ATGCCCGGGAAATTAGCTAA  [-0.39854795, -0.74053574, -0.4361593, -0.2348...\n",
       "2      ATGAAATTTCCCGGGTTTAA  [-0.037209284, -0.41736507, 0.18577996, 0.4326...\n",
       "3   ATGATATATATATGCGCGCGCGC  [-0.43701595, -0.052947313, -0.2544344, 0.2598...\n",
       "4   ATGCGCGTATATATATATAGTAG  [-0.8236446, -0.2006473, -0.6237884, 0.6729365..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_path = \"./pretrained_models/mrna_3utr_model_p2_cp99900_best\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_path, local_files_only=True)\n",
    "model_3utr = AutoModel.from_pretrained(model_path, config=config, local_files_only=True)\n",
    "tokenizer_utr = AutoTokenizer.from_pretrained(\"./utr_tokenizer\") \n",
    "\n",
    "df_3utr = build_UTR_embedding_dataframe(\n",
    "    region=\"3utr\",\n",
    "    mode=\"mean\",\n",
    "    tokenizer=tokenizer_utr,\n",
    "    model=model_3utr,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "display(df_3utr)\n",
    "print(len(df_3utr['embedding'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15c7f4-668e-4605-bc76-a276082b8ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
